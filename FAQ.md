# ❓ よくある質問（FAQ）

## 🎯 このドキュメントについて

日本語話者認識システム（単一話者認識・複数話者分析・手動話者分離対応）について、ユーザーから頻繁に寄せられる質問をまとめました。

## 📋 目次

- [基本的な質問](#-基本的な質問)
- [話者管理について](#-話者管理について)
- [複数話者分析について](#-複数話者分析について)
- [手動話者分離について](#-手動話者分離について)
- [背景データ・埋め込みについて](#-背景データ埋め込みについて)
- [技術的な質問](#-技術的な質問)
- [トラブルシューティング](#-トラブルシューティング)
- [性能・精度について](#-性能精度について)

## 🤔 基本的な質問

### Q1: このアプリは何ができるの？

**A: 単一話者認識、複数話者分析、手動話者分離ができます**

#### 🎤 単一話者認識
```
入力: 1人の音声ファイル
出力: 田中部長の声です（信頼度: 89.2%）
```

#### 🎭 複数話者分析
```
入力: 複数人の会議録音
出力: 
0-3秒: 田中部長 (89.2%)
3-6秒: 佐藤課長 (84.7%)
6-9秒: 田中部長 (91.5%)
```

#### 🔧 手動話者分離（NEW!）
```
入力: 任意の音声ファイル
操作: リアルタイム音声プレーヤーで範囲選択
出力: 精密なセグメント別話者割当
```

用途:
- 会議録音の発言者特定（時系列分析）
- コールセンターの品質管理
- セキュリティシステム
- 研究・教育用途

### Q2: 料金はかかる？

**A: 無料です**

- ソフトウェア本体: **無料**（オープンソース）
- モデル: **無料**（Hugging Face Hub）
- データセット: **無料**（JVS, Common Voice）

### Q3: インターネット接続は必要？

**A: 初回と複数話者分析に必要です**

#### 単一話者認識
- **初回**: モデルダウンロードのため必要
- **2回目以降**: オフラインで動作可能

#### 複数話者分析
- **初回**: pyannote.audioモデルダウンロードに必要
- **Hugging Face認証**: HF_TOKEN設定が必要
- **利用規約同意**: pyannote/speaker-diarization

#### 背景データ
- **Common Voice**: ストリーミング取得時のみ必要

### Q4: どのOSで動く？

**A: Windows, Mac, Linuxで動作します**

- Windows 10/11
- macOS 10.15以降
- Ubuntu 18.04以降
- Python 3.8以上が必要

## 👥 話者管理について

### Q5: 何人まで登録できる？

**A: 理論上は無制限ですが、実用的には50人程度まで**

推奨人数:
- **小規模**: 5〜10人（高速・高精度）
- **中規模**: 20〜30人（実用的）
- **大規模**: 50人以上（処理時間増加）

### Q6: JVSみたいに埋め込み作成は必要？

**A: 不要です！フォルダ操作だけで完了**

| JVS/Common Voice | 普通の話者 |
|------------------|------------|
| 30分〜1時間の処理 | 1-2分（初回のみ） |
| 専用スクリプト実行 | フォルダにコピーだけ |
| .npzファイル作成 | 音声ファイルそのまま |

### Q7: 音声ファイルはいくつ必要？

**A: 1人あたり2〜5個が目安**

```
✅ 推奨:
- 最低: 2個
- 理想: 3〜5個
- 上限: 10個程度

推奨理由:
- 2個未満: 精度不足
- 10個超: 処理時間増加、効果頭打ち
```

### Q8: 音声の長さはどのくらい？

**A: 2秒〜30秒がシステム制限、3秒〜15秒が最適**

```
✅ 最適な長さ:
- 挨拶: 「おはようございます。田中です」(5秒)
- 自己紹介: 「営業部の佐藤と申します」(6秒)
- 業務連絡: 「会議の件で連絡しました」(7秒)

❌ 避けるべき:
- 短すぎ: 「はい」(1秒) → システム制限未満
- 長すぎ: 35秒の長いスピーチ → システム制限超過
```

### Q9: どんな音声が良い？

**A: 自然な話し方の音声**

```
✅ 良い音声:
- 普通の会話
- 電話での応対
- 会議での発言
- 自己紹介

❌ 避けるべき音声:
- 歌声
- ささやき声
- 叫び声
- 音楽やノイズ混入
- 複数人が同時に話している
```

## 🎭 複数話者分析について

### Q10: 複数話者分析って何？

**A: 複数人が話している音声を時系列で誰が話しているかを分析する機能です**

処理フロー:
```
1. ダイアライゼーション: pyannote.audioで「いつ誰が話しているか」検出
2. セグメント認識: 各時間帯の話者を登録話者から識別
3. 結果表示: タイムライン形式で可視化
```

### Q11: Hugging Face Tokenって何？必要？

**A: pyannote.audioを使用するために必要な認証情報です**

取得方法:
```
1. https://huggingface.co/settings/tokens でアカウント作成
2. トークン作成（read権限）
3. https://huggingface.co/pyannote/speaker-diarization で利用規約同意
4. .envファイルにHF_TOKEN=your_token_hereを設定
```

### Q12: 何人まで同時に分析できる？

**A: 設定により1〜10人まで**

```
推奨設定:
- 会議: 2〜5人
- インタビュー: 2〜3人
- パネルディスカッション: 3〜6人

制限事項:
- 最大話者数: 10人（config.jsonで調整可能）
- 重複発話: 部分的対応
- 最小セグメント長: 0.5秒（調整可能）
```

### Q13: 複数話者分析の精度はどのくらい？

**A: 音声品質と話者数により変動します**

```
理想的条件（クリアな音声、明確な話者分離）:
- 2人: 85〜95%
- 3〜4人: 75〜90%
- 5人以上: 60〜80%

困難な条件（ノイズ、重複発話）:
- 精度が10〜20%低下
```

### Q14: タイムラインチャートって何？

**A: 話者の発話パターンを時系列で可視化する機能です**

表示内容:
```
📊 ダイアライゼーションタブ:
- pyannote.audioの生の分離結果
- SPEAKER_00, SPEAKER_01形式

👥 話者別タイムラインタブ:
- 認識結果ベースの表示
- 実際の話者名（田中部長など）
- 話者別統計情報
```

## 🔧 手動話者分離について

### Q15: 手動話者分離って何が違うの？

**A: リアルタイム音声同期による精密な手動セグメント作成機能です**

比較:
```
複数話者分析（自動）:
✅ 自動処理
❌ 精度は音声品質に依存
❌ 誤分離の修正困難

手動話者分離（手動）:
✅ 100%正確なセグメント作成
✅ リアルタイム音声同期
✅ 直感的な操作
❌ 手動作業が必要
```

### Q16: audixって何？

**A: streamlit-advanced-audioライブラリによる高度音声プレーヤーです**

特徴:
```
🎵 真のリアルタイム同期
- currentTime & isPlayingの完全連携
- <100ms の低遅延

🎯 WaveSurfer.js基盤
- プロ級の波形表示
- ドラッグ&ドロップ範囲選択
- カスタムリージョン表示

📍 精密操作
- 0.1秒単位の正確な時間指定
- 数値入力とドラッグの両対応
- 即座のフィードバック
```

### Q17: 手動分離はいつ使うべき？

**A: 高精度が必要な場合や自動分析が困難な音声**

推奨ケース:
```
✅ 重複発話が多い
✅ 音質が悪い・ノイズ多
✅ 話者の声が似ている
✅ 精密な分析が必要
✅ 少量の音声（効率性重視でない）

避けるべきケース:
❌ 大量の音声処理
❌ 明確に分離された音声
❌ 時間効率を最重視
```

### Q18: 範囲選択がうまくできない

**A: 2つの方法を使い分けてください**

**方法A: ドラッグ選択**
```
1. 波形プレーヤー上でマウスドラッグ
2. 選択範囲がリアルタイムで表示
3. 音声再生で範囲確認
4. 「➕ セグメント追加」ボタン
```

**方法B: 数値入力**
```
1. 右パネルで開始・終了時間を入力
2. 「📍 この範囲を選択」ボタン
3. より正確な時間指定が可能
4. 「➕ セグメント追加」ボタン
```

トラブル対処:
```bash
# audixが表示されない場合
pip install streamlit-advanced-audio>=0.1.0

# ドラッグ操作が効かない場合
- ブラウザのJavaScriptを有効化
- ページを再読み込み
```

## 🗂️ 背景データ・埋め込みについて

### Q19: 背景データって何？必要？

**A: 識別精度を向上させるデータです。オプションですが推奨**

役割:
```
通常の識別:
話者A: 0.51  ← 微妙な差
話者B: 0.49

背景データ使用時（AS-Norm）:
話者A: 2.3   ← 明確な差
話者B: -0.8
```

効果: **識別精度が大幅向上**

### Q16: JVSと Common Voice の違いは？

**A: データの特徴と取得方法が違います**

| 項目 | JVS | Common Voice |
|------|-----|--------------|
| **話者数** | 100名 | 数千名 |
| **音質** | 高品質（スタジオ録音） | 一般的（一般投稿） |
| **取得** | 手動ダウンロード必要 | 自動ストリーミング |
| **ライセンス** | 再配布禁止 | CC0（自由使用） |
| **処理時間** | 30分〜1時間 | 15分〜30分 |

両方使用すると最高精度になります。

### Q17: 埋め込みファイルのサイズは？

**A: 音声ファイルより大幅に小さくなります**

```
JVS:
元データ: 2.7GB（音声ファイル）
埋め込み: 3.8MB（.npzファイル）
圧縮率: 99.9%

Common Voice:
ストリーミング: 全量DL不要
埋め込み: 19MB（.npzファイル）
```

### Q18: ダミーファイルって何？

**A: 動作確認用の仮データです**

用途:
- システムの動作確認
- 認証なしでの体験
- 開発・テスト用

注意:
```
⚠️ ダミーファイルは精度が劣ります
本格運用では実データを使用してください
```

## 🔧 技術的な質問

### Q19: GPU は必要？

**A: 必須ではありませんが、あると高速化されます**

処理時間の比較:
```
JVS埋め込み作成:
- CPU: 30分〜1時間
- GPU: 10分〜20分

単一話者識別:
- CPU: 5秒〜10秒
- GPU: 1秒〜3秒

複数話者分析:
- CPU: 30秒〜2分
- GPU: 10秒〜30秒
```

対応GPU:
- NVIDIA GPU（CUDA 12.6対応）
- Apple Silicon（MPS対応）

### Q20: メモリはどのくらい必要？

**A: 8GB以上推奨（複数話者分析使用時）**

```
単一話者認識: 4GB（快適）
複数話者分析: 8GB以上（推奨）
大規模運用: 16GB以上（多数話者登録時）
```

### Q21: 対応している音声形式は？

**A: 主要な音声形式に対応**

```
✅ 対応形式:
- WAV（推奨）
- MP3
- FLAC
- M4A
- OGG

❌ 非対応:
- MP4（動画）
- AVI（動画）
- WMA
```

### Q22: セキュリティは大丈夫？

**A: ローカル環境で完結するため安全です**

特徴:
- **データ外部送信なし**: 全てローカル処理
- **インターネット不要**: 初回後はオフライン可
- **プライバシー保護**: 音声データは手元に残る

## 🐛 トラブルシューティング

### Q23: モデル初期化でエラーが出る

**A: 以下を順番に確認してください**

```
1. インターネット接続確認
2. Python環境確認: python --version
3. ライブラリ再インストール: pip install -r requirements.txt --upgrade
4. アプリ再起動: Ctrl+C → streamlit run app.py
```

### Q24: 複数話者分析でHugging Face認証エラー

**A: 以下の手順で解決**

```
エラー: Authentication error / Token required
解決法:
1. https://huggingface.co/settings/tokens でトークン作成
2. https://huggingface.co/pyannote/speaker-diarization で利用規約同意
3. .envファイルにHF_TOKEN=your_token_hereを設定
4. アプリ再起動
```

### Q25: 話者が表示されない

**A: フォルダ構造と音声ファイルを確認**

チェック項目:
```
✅ フォルダの場所: enroll\話者名\
✅ 音声形式: WAV, MP3など対応形式
✅ 音声長: 2秒以上30秒以下
✅ ファイル破損: 他のプレイヤーで再生可能か
```

### Q26: セグメントが検出されない

**A: 音声品質と設定を確認**

```
音声確認:
- 複数話者が明確に分離されているか
- 音声が2秒以上あるか
- ノイズが少ないか

設定調整:
- 話者数設定を調整（最小〜最大）
- 最小セグメント長を短縮（config.json）
```

### Q27: 識別精度が悪い

**A: 以下の方法で改善できます**

短期的改善:
```
1. 音声ファイル追加（1人あたり3〜5個）
2. 音質改善（ノイズ除去）
3. 音声長調整（3〜15秒）
4. 録音環境の多様化
```

長期的改善:
```
1. JVS埋め込み作成
2. Common Voice埋め込み作成
3. config.json調整（AS-Norm設定）
```

## 📊 性能・精度について

### Q28: 単一話者認識の精度はどのくらい？

**A: 条件により大きく変わります**

一般的な精度:
```
基本設定（背景データなし）:
- 5人以下: 85〜95%
- 10人程度: 75〜90%
- 20人以上: 60〜80%

AS-Norm使用時:
- 5人以下: 90〜98%
- 10人程度: 85〜95%
- 20人以上: 75〜90%
```

### Q29: 複数話者分析の処理時間は？

**A: 音声長と話者数によります**

```
処理時間（GPU使用時）:
- 10秒音声: 5〜10秒
- 1分音声: 15〜30秒
- 5分音声: 1〜2分

CPU使用時:
- 上記の2〜3倍程度
```

### Q30: リアルタイム処理はできる？

**A: 準リアルタイムは可能です**

```
単一話者識別: 1秒〜10秒
複数話者分析: 10秒〜2分
（音声長や環境による）

ライブストリーミング:
現在未対応（ファイルアップロードのみ）
```

### Q31: 多言語対応は？

**A: 現在は日本語のみです**

理由:
- ECAPAモデル: 多言語対応
- pyannote.audio: 多言語対応
- 背景データ: 日本語（JVS, Common Voice日本語）
- システム設計: 日本語特化

他言語での使用:
```
可能ですが：
- 背景データなしでの動作
- 精度低下の可能性
```

### Q32: 商用利用は可能？

**A: ライセンスを確認した上で可能です**

各コンポーネントのライセンス:
```
✅ システム本体: MIT（商用可）
✅ SpeechBrain: Apache 2.0（商用可）
✅ pyannote.audio: MIT（商用可）
⚠️ JVS: 研究用途のみ（商用は要確認）
✅ Common Voice: CC0（商用可）
```

推奨: 商用利用前に各ライセンスを詳細確認

## 🎯 その他

### Q33: 更新やアップデートは？

**A: GitHubで最新版を公開予定**

更新内容:
- バグ修正
- 機能追加（新機能: 複数話者分析など）
- 性能改善
- 新しいモデル対応

### Q34: コントリビューション（貢献）は可能？

**A: 大歓迎です！**

貢献方法:
- バグ報告: GitHub Issues
- 機能要望: GitHub Issues
- コード改善: Pull Request
- ドキュメント改善: Pull Request

### Q35: サポートはある？

**A: コミュニティベースのサポート**

サポート窓口:
- **GitHub Issues**: 技術的問題
- **README.md**: 基本的な使い方
- **FAQ.md**: よくある質問（このファイル）

---

## 📞 まだ解決しない場合

1. **ドキュメント確認**:
   - [README.md](README.md)
   - [SPEAKER_MANAGEMENT_GUIDE.md](SPEAKER_MANAGEMENT_GUIDE.md)
   - [QUICK_START_GUIDE.md](QUICK_START_GUIDE.md)

2. **GitHub Issues**で質問投稿

3. **ログ確認**: アプリ画面のエラーメッセージを詳細に記録

この FAQ で解決しない問題があれば、お気軽にお問い合わせください！