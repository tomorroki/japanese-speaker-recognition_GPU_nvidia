"""
æ—¥æœ¬èªè©±è€…èªè­˜ã‚·ã‚¹ãƒ†ãƒ  - Streamlit UI
"""

import streamlit as st
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import time
import os
import tempfile
from pathlib import Path
from typing import Dict, Any, Optional

from enhanced_speaker_recognition import JapaneseSpeakerRecognizer, RecognitionResult
from dataset_manager import DatasetManager

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(
    page_title="æ—¥æœ¬èªè©±è€…èªè­˜ã‚·ã‚¹ãƒ†ãƒ ",
    page_icon="ğŸ¤",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã®åˆæœŸåŒ–
if 'recognizer' not in st.session_state:
    st.session_state.recognizer = None
if 'model_loaded' not in st.session_state:
    st.session_state.model_loaded = False
if 'speakers_enrolled' not in st.session_state:
    st.session_state.speakers_enrolled = 0

# ãƒ¡ã‚¤ãƒ³é–¢æ•°
def main():
    st.title("ğŸ¤ æ—¥æœ¬èªè©±è€…èªè­˜ã‚·ã‚¹ãƒ†ãƒ ")
    st.markdown("### SpeechBrain + ECAPA-TDNNã«ã‚ˆã‚‹é«˜ç²¾åº¦è©±è€…è­˜åˆ¥")
    
    # ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡
    setup_sidebar()
    
    # ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„
    if st.session_state.model_loaded:
        display_main_content()
    else:
        display_welcome_page()

def setup_sidebar():
    """ã‚µã‚¤ãƒ‰ãƒãƒ¼ã®è¨­å®š"""
    st.sidebar.header("ğŸ”§ ã‚·ã‚¹ãƒ†ãƒ åˆ¶å¾¡")
    
    # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    if st.sidebar.button("ğŸš€ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–", type="primary"):
        initialize_system()
    
    # ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹è¡¨ç¤º
    st.sidebar.subheader("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çŠ¶æ…‹")
    
    status_color = "ğŸŸ¢" if st.session_state.model_loaded else "ğŸ”´"
    st.sidebar.write(f"{status_color} ãƒ¢ãƒ‡ãƒ«: {'èª­ã¿è¾¼ã¿æ¸ˆã¿' if st.session_state.model_loaded else 'æœªèª­ã¿è¾¼ã¿'}")
    st.sidebar.write(f"ğŸ‘¥ ç™»éŒ²è©±è€…æ•°: {st.session_state.speakers_enrolled}")
    
    if st.session_state.model_loaded:
        
        # åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
        st.sidebar.subheader("ğŸ’¾ åŸ‹ã‚è¾¼ã¿ã‚­ãƒ£ãƒƒã‚·ãƒ¥")
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çŠ¶æ…‹è¡¨ç¤º
        if st.session_state.recognizer:
            cache_exists = os.path.exists("enrolled_speakers_embeddings.npz")
            cache_color = "ğŸŸ¢" if cache_exists else "ğŸ”´"
            st.sidebar.write(f"{cache_color} ã‚­ãƒ£ãƒƒã‚·ãƒ¥: {'å­˜åœ¨' if cache_exists else 'æœªä½œæˆ'}")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†ãƒœã‚¿ãƒ³
            col1, col2 = st.sidebar.columns(2)
            with col1:
                if st.button("ğŸ’¾ ä¿å­˜") and st.session_state.recognizer.speaker_embeddings:
                    if st.session_state.recognizer.save_speaker_embeddings():
                        st.sidebar.success("âœ… ä¿å­˜å®Œäº†")
                    else:
                        st.sidebar.error("âŒ ä¿å­˜å¤±æ•—")
            
            with col2:
                if st.button("ğŸ—‘ï¸ å‰Šé™¤") and cache_exists:
                    try:
                        os.remove("enrolled_speakers_embeddings.npz")
                        st.sidebar.success("âœ… å‰Šé™¤å®Œäº†")
                        st.rerun()
                    except Exception as e:
                        st.sidebar.error(f"âŒ å‰Šé™¤å¤±æ•—: {e}")
        
        
        # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±è¡¨ç¤º
        if st.sidebar.button("â„¹ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±"):
            show_system_info()

def initialize_system():
    """ã‚·ã‚¹ãƒ†ãƒ ã®åˆæœŸåŒ–"""
    with st.spinner("ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­..."):
        try:
            # RecognizeråˆæœŸåŒ–
            recognizer = JapaneseSpeakerRecognizer()
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
            status_text.text("ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            progress_bar.progress(25)
            
            if not recognizer.initialize_model():
                st.error("âŒ ãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            # è©±è€…ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ§‹ç¯‰
            status_text.text("è©±è€…ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’æ§‹ç¯‰ä¸­...")
            progress_bar.progress(50)
            
            enrolled_count = recognizer.build_speaker_database()
            
            # èƒŒæ™¯ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
            status_text.text("èƒŒæ™¯ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ä¸­...")
            progress_bar.progress(75)
            
            recognizer.build_background_model()
            
            # å®Œäº†
            progress_bar.progress(100)
            status_text.text("åˆæœŸåŒ–å®Œäº†ï¼")
            
            # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹æ›´æ–°
            st.session_state.recognizer = recognizer
            st.session_state.model_loaded = True
            st.session_state.speakers_enrolled = enrolled_count
            
            st.success(f"âœ… ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†ï¼{enrolled_count}åã®è©±è€…ã‚’ç™»éŒ²ã—ã¾ã—ãŸ")
            time.sleep(1)
            st.rerun()
            
        except Exception as e:
            st.error(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {str(e)}")


def show_system_info():
    """ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®è¡¨ç¤º"""
    if st.session_state.recognizer is None:
        st.error("ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    info = st.session_state.recognizer.get_system_info()
    
    st.sidebar.subheader("ğŸ” è©³ç´°æƒ…å ±")
    for key, value in info.items():
        st.sidebar.write(f"**{key}**: {value}")

def display_welcome_page():
    """ã‚¦ã‚§ãƒ«ã‚«ãƒ ãƒšãƒ¼ã‚¸ã®è¡¨ç¤º"""
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        st.markdown("""
        ## ğŸ¯ ã‚·ã‚¹ãƒ†ãƒ æ¦‚è¦
        
        ã“ã®ã‚·ã‚¹ãƒ†ãƒ ã¯**SpeechBrain**ã¨**ECAPA-TDNN**ã‚’ä½¿ç”¨ã—ãŸ
        é«˜ç²¾åº¦ãªæ—¥æœ¬èªè©±è€…èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã§ã™ã€‚
        
        ### ğŸš€ ä¸»ãªæ©Ÿèƒ½
        
        - **é«˜ç²¾åº¦èªè­˜**: ECAPA-TDNNã«ã‚ˆã‚‹æœ€å…ˆç«¯ã®è©±è€…åŸ‹ã‚è¾¼ã¿
        - **èƒŒæ™¯è©±è€…é™¤å¤–**: JVSãƒ»Common Voiceè©±è€…ã®è‡ªå‹•é™¤å¤–
        - **ã‚¹ã‚³ã‚¢æ­£è¦åŒ–**: èƒŒæ™¯ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã‚‹èªè­˜ç²¾åº¦å‘ä¸Š
        - **ç›´æ„Ÿçš„UI**: Streamlitã«ã‚ˆã‚‹ä½¿ã„ã‚„ã™ã„ã‚¤ãƒ³ã‚¿ãƒ¼face
        
        ### ğŸ“‹ ä½¿ç”¨æ–¹æ³•
        
        1. **ã‚µã‚¤ãƒ‰ãƒãƒ¼**ã®ã€ŒğŸš€ ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã€ã‚’ã‚¯ãƒªãƒƒã‚¯
        2. éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦è©±è€…è­˜åˆ¥
        3. çµæœã¨ã‚¹ã‚³ã‚¢è©³ç´°ã‚’ç¢ºèª
        
        ### ğŸ“ ãƒ‡ãƒ¼ã‚¿æº–å‚™
        
        ```
        enroll/
        â”œâ”€â”€ yamada_taro/     # è©±è€…1ã®ãƒ•ã‚¡ã‚¤ãƒ«
        â”œâ”€â”€ sato_hanako/     # è©±è€…2ã®ãƒ•ã‚¡ã‚¤ãƒ«
        â””â”€â”€ tanaka_jiro/     # è©±è€…3ã®ãƒ•ã‚¡ã‚¤ãƒ«
        ```
        
        ---
        
        **æº–å‚™ãŒã§ããŸã‚‰ã€ã‚µã‚¤ãƒ‰ãƒãƒ¼ã§ã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ã—ã¦ãã ã•ã„ï¼**
        """)

def display_main_content():
    """ãƒ¡ã‚¤ãƒ³ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®è¡¨ç¤º"""
    # ã‚¿ãƒ–è¨­å®š
    tab1, tab2, tab3, tab4 = st.tabs([
        "ğŸ¤ å˜ä¸€è©±è€…è­˜åˆ¥", 
        "ğŸ­ è¤‡æ•°è©±è€…åˆ†æ", 
        "ğŸ‘¥ è©±è€…ç®¡ç†", 
        "ğŸ“Š çµ±è¨ˆæƒ…å ±"
    ])
    
    with tab1:
        display_recognition_tab()
    
    with tab2:
        display_diarization_tab()
    
    with tab3:
        display_speaker_management_tab()
    
    with tab4:
        display_statistics_tab()

def display_recognition_tab():
    """å˜ä¸€è©±è€…è­˜åˆ¥ã‚¿ãƒ–"""
    st.header("ğŸ¤ å˜ä¸€è©±è€…è­˜åˆ¥")
    st.caption("1åã®è©±è€…ã‚’ç™»éŒ²æ¸ˆã¿è©±è€…ã‹ã‚‰è­˜åˆ¥ã—ã¾ã™")
    
    if st.session_state.speakers_enrolled == 0:
        st.warning("âš ï¸ ç™»éŒ²ã•ã‚ŒãŸè©±è€…ãŒã„ã¾ã›ã‚“ã€‚enrollãƒ•ã‚©ãƒ«ãƒ€ã«éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„ã€‚")
        return
    
    # è¡¨ç¤ºè¨­å®š
    st.subheader("ğŸ›ï¸ è¡¨ç¤ºè¨­å®š")
    col1, col2 = st.columns(2)
    
    with col1:
        show_jvs = st.checkbox(
            "ğŸ—¾ JVSè©±è€…ã‚’çµæœã«è¡¨ç¤º", 
            value=st.session_state.recognizer.config["ui"]["show_jvs_in_results"] if st.session_state.recognizer else True,
            help="èªè­˜çµæœã®ãƒˆãƒƒãƒ—10ã«JVSè©±è€…ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹"
        )
    
    with col2:
        show_cv = st.checkbox(
            "ğŸŒ Common Voiceè©±è€…ã‚’çµæœã«è¡¨ç¤º",
            value=st.session_state.recognizer.config["ui"]["show_common_voice_in_results"] if st.session_state.recognizer else False,
            help="èªè­˜çµæœã®ãƒˆãƒƒãƒ—10ã«Common Voiceè©±è€…ã‚’å«ã‚ã‚‹ã‹ã©ã†ã‹"
        )
    
    # è¨­å®šã‚’ä¸€æ™‚çš„ã«ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã«ä¿å­˜
    if 'show_jvs_in_results' not in st.session_state:
        st.session_state.show_jvs_in_results = show_jvs
    if 'show_common_voice_in_results' not in st.session_state:
        st.session_state.show_common_voice_in_results = show_cv
    
    # è¨­å®šãŒå¤‰æ›´ã•ã‚ŒãŸå ´åˆã®å‡¦ç†
    if (st.session_state.show_jvs_in_results != show_jvs or 
        st.session_state.show_common_voice_in_results != show_cv):
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°
        st.session_state.show_jvs_in_results = show_jvs
        st.session_state.show_common_voice_in_results = show_cv
        
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚‚æ›´æ–°
        if st.session_state.recognizer:
            st.session_state.recognizer.config["ui"]["show_jvs_in_results"] = show_jvs
            st.session_state.recognizer.config["ui"]["show_common_voice_in_results"] = show_cv
            
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
            try:
                import json
                with open("config.json", 'w', encoding='utf-8') as f:
                    json.dump(st.session_state.recognizer.config, f, indent=2, ensure_ascii=False)
            except Exception as e:
                st.error(f"è¨­å®šä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    else:
        # å¤‰æ›´ãŒãªã„å ´åˆã¯ã‚»ãƒƒã‚·ãƒ§ãƒ³çŠ¶æ…‹ã‚’æ›´æ–°ã™ã‚‹ã ã‘
        st.session_state.show_jvs_in_results = show_jvs
        st.session_state.show_common_voice_in_results = show_cv
    
    st.divider()
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['wav', 'mp3', 'flac', 'm4a', 'ogg'],
        help="å¯¾å¿œå½¢å¼: WAV, MP3, FLAC, M4A, OGG"
    )
    
    if uploaded_file is not None:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix=Path(uploaded_file.name).suffix) as tmp_file:
            tmp_file.write(uploaded_file.getvalue())
            tmp_path = tmp_file.name
        
        try:
            # éŸ³å£°æƒ…å ±è¡¨ç¤º
            st.subheader("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±")
            col1, col2 = st.columns(2)
            
            with col1:
                st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«å**: {uploaded_file.name}")
                st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: {uploaded_file.size / 1024:.1f} KB")
            
            with col2:
                # éŸ³å£°ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼
                st.audio(uploaded_file.getvalue())
            
            # è­˜åˆ¥å®Ÿè¡Œ
            if st.button("ğŸ” è©±è€…è­˜åˆ¥é–‹å§‹", type="primary"):
                perform_recognition(tmp_path)
        
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)

def perform_recognition(audio_path: str):
    """è©±è€…è­˜åˆ¥ã®å®Ÿè¡Œ"""
    with st.spinner("éŸ³å£°ã‚’è§£æä¸­..."):
        try:
            # é€²æ—è¡¨ç¤º
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            status_text.text("éŸ³å£°ã‚’å‰å‡¦ç†ä¸­...")
            progress_bar.progress(25)
            
            # è©±è€…è­˜åˆ¥å®Ÿè¡Œ
            status_text.text("è©±è€…åŸ‹ã‚è¾¼ã¿ã‚’æŠ½å‡ºä¸­...")
            progress_bar.progress(50)
            
            result = st.session_state.recognizer.recognize_speaker(audio_path)
            
            status_text.text("ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ä¸­...")
            progress_bar.progress(75)
            
            if result is None:
                st.error("âŒ è©±è€…è­˜åˆ¥ã«å¤±æ•—ã—ã¾ã—ãŸ")
                return
            
            progress_bar.progress(100)
            status_text.text("è­˜åˆ¥å®Œäº†ï¼")
            
            # çµæœè¡¨ç¤º
            display_recognition_result(result)
            
        except Exception as e:
            st.error(f"âŒ è­˜åˆ¥ã‚¨ãƒ©ãƒ¼: {str(e)}")

def display_recognition_result(result: RecognitionResult):
    """èªè­˜çµæœã®è¡¨ç¤º"""
    st.subheader("ğŸ¯ è­˜åˆ¥çµæœ")
    
    # è¡¨ç¤ºè¨­å®šã«åŸºã¥ã„ã¦ã‚¹ã‚³ã‚¢ã‚’ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
    show_jvs = getattr(st.session_state, 'show_jvs_in_results', True)
    show_cv = getattr(st.session_state, 'show_common_voice_in_results', False)
    
    # å…¨ã‚¹ã‚³ã‚¢ã‹ã‚‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°æ¸ˆã¿ã‚¹ã‚³ã‚¢ã‚’å–å¾—
    if result.all_scores:
        filtered_scores = st.session_state.recognizer.filter_scores_for_display(
            result.all_scores, show_jvs, show_cv
        )
        
        # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œã®æœ€ä¸Šä½è©±è€…ã‚’æ±ºå®š
        filtered_best_speaker = None
        filtered_best_score = None
        if filtered_scores:
            filtered_best_speaker = max(filtered_scores, key=filtered_scores.get)
            filtered_best_score = filtered_scores[filtered_best_speaker]
    else:
        filtered_scores = {}
        filtered_best_speaker = None
        filtered_best_score = None
    
    # è¡¨ç¤ºã™ã‚‹è©±è€…ã¨ä¿¡é ¼åº¦ã‚’æ±ºå®š
    display_speaker = result.speaker_id
    display_confidence = result.confidence
    display_raw_score = result.raw_score
    
    # ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®šã§ã‚ªãƒªã‚¸ãƒŠãƒ«ã®æœ€ä¸Šä½è©±è€…ãŒé™¤å¤–ã•ã‚Œã‚‹å ´åˆ
    if filtered_best_speaker and filtered_best_speaker != result.speaker_id:
        # JVSè©±è€…ãŒé™¤å¤–ã•ã‚Œã‚‹å ´åˆã®åˆ¤å®š
        is_original_jvs = st.session_state.recognizer.dataset_manager.is_jvs_speaker(result.speaker_id)
        is_original_cv = st.session_state.recognizer.dataset_manager.is_common_voice_speaker(result.speaker_id)
        
        if (is_original_jvs and not show_jvs) or (is_original_cv and not show_cv):
            display_speaker = filtered_best_speaker
            display_raw_score = filtered_best_score
            # ä¿¡é ¼åº¦ã¯åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ã§è¨ˆç®—ï¼ˆç°¡ç•¥åŒ–ï¼‰
            display_confidence = filtered_best_score
    
    # ãƒ¡ã‚¤ãƒ³çµæœ
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric(
            label="è­˜åˆ¥ã•ã‚ŒãŸè©±è€…",
            value=display_speaker,
            delta=None
        )
    
    with col2:
        confidence_color = "ğŸŸ¢" if display_confidence > 0.5 else "ğŸŸ¡" if display_confidence > 0.25 else "ğŸ”´"
        st.metric(
            label="ä¿¡é ¼åº¦",
            value=f"{display_confidence:.3f}",
            delta=confidence_color
        )
    
    with col3:
        st.metric(
            label="ç”Ÿã‚¹ã‚³ã‚¢",
            value=f"{display_raw_score:.3f}"
        )
    
    # ã—ãã„å€¤ãƒã‚§ãƒƒã‚¯
    threshold = st.session_state.recognizer.threshold
    if display_confidence > threshold:
        st.success(f"âœ… ä¿¡é ¼åº¦ãŒã—ãã„å€¤({threshold:.3f})ã‚’ä¸Šå›ã‚Šã¾ã—ãŸ")
    else:
        st.warning(f"âš ï¸ ä¿¡é ¼åº¦ãŒã—ãã„å€¤({threshold:.3f})ã‚’ä¸‹å›ã‚Šã¾ã—ãŸ")
    
    # è©³ç´°ã‚¹ã‚³ã‚¢è¡¨ç¤ºï¼ˆãƒˆãƒƒãƒ—10ï¼‰
    if filtered_scores:
        st.subheader("ğŸ“Š ãƒˆãƒƒãƒ—10è©±è€…ã‚¹ã‚³ã‚¢")
        filter_info = []
        if not show_jvs:
            filter_info.append("JVSè©±è€…ã‚’é™¤å¤–")
        if not show_cv:
            filter_info.append("Common Voiceè©±è€…ã‚’é™¤å¤–")
        
        caption = f"ä¸Šä½{len(filtered_scores)}åã®é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢"
        if filter_info:
            caption += f" ({', '.join(filter_info)})"
        st.caption(caption)
        
        display_score_chart(filtered_scores, display_speaker)
    
    # æ­£è¦åŒ–ã‚¹ã‚³ã‚¢æƒ…å ±
    if result.normalized_score is not None:
        st.subheader("ğŸ”§ ã‚¹ã‚³ã‚¢æ­£è¦åŒ–æƒ…å ±")
        col1, col2 = st.columns(2)
        
        with col1:
            st.write(f"**æ­£è¦åŒ–å‰**: {result.raw_score:.3f}")
            st.write(f"**æ­£è¦åŒ–å¾Œ**: {result.normalized_score:.3f}")
        
        with col2:
            improvement = result.normalized_score - result.raw_score
            improvement_color = "ğŸŸ¢" if improvement > 0 else "ğŸ”´"
            st.write(f"**æ”¹å–„åº¦**: {improvement_color} {improvement:+.3f}")

def display_score_chart(scores: Dict[str, float], best_speaker: str):
    """ã‚¹ã‚³ã‚¢ãƒãƒ£ãƒ¼ãƒˆã®è¡¨ç¤º"""
    # ãƒ‡ãƒ¼ã‚¿æº–å‚™
    speakers = list(scores.keys())
    score_values = list(scores.values())
    colors = ['red' if speaker == best_speaker else 'lightblue' for speaker in speakers]
    
    # æ¨ªæ£’ã‚°ãƒ©ãƒ•
    fig = go.Figure(data=[
        go.Bar(
            y=speakers,
            x=score_values,
            orientation='h',
            marker_color=colors,
            text=[f"{score:.3f}" for score in score_values],
            textposition='auto'
        )
    ])
    
    fig.update_layout(
        title="è©±è€…åˆ¥é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢",
        xaxis_title="é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢",
        yaxis_title="è©±è€…ID",
        height=max(300, len(speakers) * 40),
        showlegend=False
    )
    
    st.plotly_chart(fig, use_container_width=True)

def display_speaker_management_tab():
    """è©±è€…ç®¡ç†ã‚¿ãƒ–"""
    st.header("ğŸ‘¥ è©±è€…ç®¡ç†")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†
    dataset_manager = DatasetManager()
    
    # çµ±è¨ˆæƒ…å ±
    stats = dataset_manager.get_speaker_statistics("enroll")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("ç·è©±è€…æ•°", stats["total_speakers"])
    
    with col2:
        st.metric("æœ‰åŠ¹è©±è€…æ•°", stats["valid_speakers"])
    
    with col3:
        st.metric("é™¤å¤–è©±è€…æ•°", stats["excluded_speakers"])
    
    with col4:
        st.metric("éŸ³å£°ãªã—è©±è€…æ•°", stats["speakers_with_no_audio"])
    
    # è©±è€…ãƒªã‚¹ãƒˆè¡¨ç¤º
    if os.path.exists("enroll"):
        st.subheader("ğŸ“‹ è©±è€…ä¸€è¦§")
        
        speaker_data = []
        for speaker_id in os.listdir("enroll"):
            speaker_path = os.path.join("enroll", speaker_id)
            if not os.path.isdir(speaker_path):
                continue
            
            # éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«æ•°ã‚’ã‚«ã‚¦ãƒ³ãƒˆ
            audio_files = dataset_manager._get_audio_files(speaker_path)
            
            # çŠ¶æ…‹åˆ¤å®š
            if dataset_manager.should_exclude_speaker(speaker_id):
                status = "âŒ é™¤å¤–"
                reason = "èƒŒæ™¯è©±è€…"
            elif not audio_files:
                status = "âš ï¸ éŸ³å£°ãªã—"
                reason = "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ãªã—"
            else:
                status = "âœ… æœ‰åŠ¹"
                reason = "ç™»éŒ²å¯èƒ½"
            
            speaker_data.append({
                "è©±è€…ID": speaker_id,
                "éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«æ•°": len(audio_files),
                "çŠ¶æ…‹": status,
                "ç†ç”±": reason
            })
        
        if speaker_data:
            df = pd.DataFrame(speaker_data)
            st.dataframe(df, use_container_width=True)
        else:
            st.info("ğŸ“ enrollãƒ•ã‚©ãƒ«ãƒ€ã«è©±è€…ãƒ‡ãƒ¼ã‚¿ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    
    # èƒŒæ™¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±
    st.subheader("ğŸ—‚ï¸ èƒŒæ™¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæƒ…å ±")
    bg_info = dataset_manager.get_background_dataset_info()
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.write(f"**JVSè©±è€…æ•°**: {bg_info['jvs_speakers_count']}")
        st.write(f"**é™¤å¤–è¨­å®š**: {'æœ‰åŠ¹' if bg_info['exclusion_enabled'] else 'ç„¡åŠ¹'}")
    
    with col2:
        st.write(f"**Common Voiceãƒ‘ã‚¿ãƒ¼ãƒ³**: {', '.join(bg_info['common_voice_patterns'])}")

def display_statistics_tab():
    """çµ±è¨ˆæƒ…å ±ã‚¿ãƒ–"""
    st.header("ğŸ“Š ã‚·ã‚¹ãƒ†ãƒ çµ±è¨ˆæƒ…å ±")
    
    if st.session_state.recognizer is None:
        st.warning("ã‚·ã‚¹ãƒ†ãƒ ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        return
    
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±
    info = st.session_state.recognizer.get_system_info()
    
    # ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¡¨ç¤º
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.metric("ç™»éŒ²è©±è€…æ•°", info["enrolled_speakers"])
        st.metric("ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ãƒ¬ãƒ¼ãƒˆ", f"{info['sample_rate']} Hz")
    
    with col2:
        st.metric("èƒŒæ™¯ã‚µãƒ³ãƒ—ãƒ«æ•°", info["background_samples"])
        st.metric("ã—ãã„å€¤", f"{info['threshold']:.3f}")
    
    with col3:
        st.metric("ãƒ‡ãƒã‚¤ã‚¹", info["device"])
        st.metric("ã‚¹ã‚³ã‚¢æ­£è¦åŒ–", "æœ‰åŠ¹" if info["score_normalization"] else "ç„¡åŠ¹")
    
    # è©³ç´°æƒ…å ±
    st.subheader("ğŸ”§ è©³ç´°è¨­å®š")
    
    details_data = {
        "é …ç›®": [
            "ãƒ¢ãƒ‡ãƒ«å",
            "æœ€å°éŸ³å£°é•·",
            "æœ€å¤§éŸ³å£°é•·",
            "ãƒ‡ãƒã‚¤ã‚¹",
            "ã‚¹ã‚³ã‚¢æ­£è¦åŒ–",
            "èƒŒæ™¯ã‚µãƒ³ãƒ—ãƒ«æ•°"
        ],
        "å€¤": [
            info["model_name"],
            f"{info['min_duration']} ç§’",
            f"{info['max_duration']} ç§’",
            info["device"],
            "æœ‰åŠ¹" if info["score_normalization"] else "ç„¡åŠ¹",
            f"{info['background_samples']} ã‚µãƒ³ãƒ—ãƒ«"
        ]
    }
    
    details_df = pd.DataFrame(details_data)
    st.dataframe(details_df, use_container_width=True, hide_index=True)
    
    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±
    st.subheader("âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æƒ…å ±")
    
    if info["device"] == "cuda":
        st.success("ğŸš€ GPUåŠ é€ŸãŒæœ‰åŠ¹ã§ã™")
    elif info["device"] == "mps":
        st.success("ğŸ Apple Silicon GPUåŠ é€ŸãŒæœ‰åŠ¹ã§ã™")
    else:
        st.info("ğŸ’» CPUå‡¦ç†ã§å‹•ä½œä¸­ã§ã™")

def display_diarization_tab():
    """è¤‡æ•°è©±è€…åˆ†æã‚¿ãƒ–"""
    st.header("ğŸ­ è¤‡æ•°è©±è€…åˆ†æ")
    st.caption("è¤‡æ•°è©±è€…ã®éŸ³å£°ã‹ã‚‰æ™‚ç³»åˆ—ã§ã®è©±è€…èªè­˜ã‚’è¡Œã„ã¾ã™")
    
    # åˆæœŸåŒ–ç¢ºèª
    if 'multi_recognizer' not in st.session_state:
        st.session_state.multi_recognizer = None
        st.session_state.diarization_initialized = False
    
    # åˆæœŸåŒ–ãƒœã‚¿ãƒ³
    if not st.session_state.diarization_initialized:
        if st.button("ğŸš€ è¤‡æ•°è©±è€…åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–", type="primary"):
            initialize_diarization_system()
        
        st.info("""
        ğŸ’¡ **è¤‡æ•°è©±è€…åˆ†æã«ã¤ã„ã¦**
        
        ã“ã®ã‚¿ãƒ–ã§ã¯ã€è¤‡æ•°ã®è©±è€…ãŒåŒæ™‚ã«è©±ã—ã¦ã„ã‚‹éŸ³å£°ã‚’åˆ†æã—ã€ä»¥ä¸‹ã‚’è¡Œã„ã¾ã™ï¼š
        
        ğŸ“Š **ãƒ€ã‚¤ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³**: ã€Œã„ã¤èª°ãŒè©±ã—ã¦ã„ã‚‹ã‹ã€ã‚’æ¤œå‡º
        ğŸ¯ **è©±è€…è­˜åˆ¥**: å„æ™‚é–“å¸¯ã®è©±è€…ã‚’ç™»éŒ²æ¸ˆã¿è©±è€…ã‹ã‚‰ç‰¹å®š
        
        **å¿…è¦ãªæº–å‚™**:
        - Hugging Face Token ãŒ `.env` ãƒ•ã‚¡ã‚¤ãƒ«ã«è¨­å®šæ¸ˆã¿
        - pyannote.audio ã®åˆ©ç”¨è¦ç´„ã«åŒæ„æ¸ˆã¿
        """)
        return
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader(
        "è¤‡æ•°è©±è€…ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰",
        type=['wav', 'mp3', 'flac', 'm4a', 'ogg'],
        help="2åä»¥ä¸Šã®è©±è€…ãŒå«ã¾ã‚Œã‚‹éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«"
    )
    
    if uploaded_file:
        # è¨­å®š
        col1, col2 = st.columns(2)
        with col1:
            min_speakers = st.number_input("æœ€å°è©±è€…æ•°", 1, 10, 1, help="éŸ³å£°ã«å«ã¾ã‚Œã‚‹æœ€å°è©±è€…æ•°")
        with col2:
            max_speakers = st.number_input("æœ€å¤§è©±è€…æ•°", 1, 10, 5, help="éŸ³å£°ã«å«ã¾ã‚Œã‚‹æœ€å¤§è©±è€…æ•°")
        
        # éŸ³å£°æƒ…å ±è¡¨ç¤º
        st.subheader("ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±")
        col1, col2 = st.columns(2)
        
        with col1:
            st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«å**: {uploaded_file.name}")
            st.write(f"**ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º**: {uploaded_file.size / 1024:.1f} KB")
        
        with col2:
            # éŸ³å£°ãƒ—ãƒ¬ãƒ¼ãƒ¤ãƒ¼
            st.audio(uploaded_file.getvalue())
        
        # åˆ†æå®Ÿè¡Œ
        if st.button("ğŸ­ è¤‡æ•°è©±è€…åˆ†æé–‹å§‹", type="primary"):
            perform_multi_speaker_analysis(uploaded_file, min_speakers, max_speakers)

def initialize_diarization_system():
    """è¤‡æ•°è©±è€…åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
    with st.spinner("è¤‡æ•°è©±è€…åˆ†æã‚·ã‚¹ãƒ†ãƒ ã‚’åˆæœŸåŒ–ä¸­..."):
        try:
            # é€²æ—è¡¨ç¤º
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            status_text.text("ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­...")
            progress_bar.progress(25)
            
            from speaker_diarization import MultiSpeakerRecognizer
            
            status_text.text("ãƒ€ã‚¤ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãƒ¢ãƒ‡ãƒ«ã‚’åˆæœŸåŒ–ä¸­...")
            progress_bar.progress(50)
            
            recognizer = MultiSpeakerRecognizer()
            
            status_text.text("è©±è€…èªè­˜ã‚·ã‚¹ãƒ†ãƒ ã‚’çµ±åˆä¸­...")
            progress_bar.progress(75)
            
            if recognizer.initialize():
                st.session_state.multi_recognizer = recognizer
                st.session_state.diarization_initialized = True
                
                progress_bar.progress(100)
                status_text.text("åˆæœŸåŒ–å®Œäº†ï¼")
                
                st.success("âœ… è¤‡æ•°è©±è€…åˆ†æã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–å®Œäº†")
                time.sleep(1)
                st.rerun()
            else:
                st.error("âŒ åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                
        except Exception as e:
            st.error(f"âŒ åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            st.info("ğŸ’¡ Hugging Face Token ã®è¨­å®šã‚„ pyannote.audio ã®åˆ©ç”¨è¦ç´„åŒæ„ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

def perform_multi_speaker_analysis(uploaded_file, min_speakers, max_speakers):
    """è¤‡æ•°è©±è€…åˆ†æå®Ÿè¡Œ"""
    with st.spinner("è¤‡æ•°è©±è€…åˆ†æä¸­..."):
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp:
            tmp.write(uploaded_file.getvalue())
            tmp_path = tmp.name
        
        try:
            # é€²æ—è¡¨ç¤º
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            status_text.text("ãƒ€ã‚¤ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³å®Ÿè¡Œä¸­...")
            progress_bar.progress(30)
            
            # åˆ†æå®Ÿè¡Œ
            result = st.session_state.multi_recognizer.process_audio(
                tmp_path, min_speakers, max_speakers
            )
            
            status_text.text("è©±è€…èªè­˜å®Ÿè¡Œä¸­...")
            progress_bar.progress(70)
            
            progress_bar.progress(100)
            status_text.text("åˆ†æå®Œäº†ï¼")
            
            # çµæœè¡¨ç¤º
            display_multi_speaker_result(result)
            
        except Exception as e:
            st.error(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        finally:
            # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
            if os.path.exists(tmp_path):
                os.unlink(tmp_path)

def display_multi_speaker_result(result):
    """è¤‡æ•°è©±è€…åˆ†æçµæœè¡¨ç¤º"""
    st.subheader("ğŸ¯ åˆ†æçµæœ")
    
    # ã‚µãƒãƒªãƒ¼
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("æ¤œå‡ºè©±è€…æ•°", result.total_speakers)
    with col2:
        st.metric("ç·æ™‚é–“", f"{result.total_duration:.1f}ç§’")
    with col3:
        st.metric("ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°", len(result.segments))
    
    # ã‚»ã‚°ãƒ¡ãƒ³ãƒˆè©³ç´°
    if result.segments:
        st.subheader("ğŸ“‹ æ™‚ç³»åˆ—ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ")
        
        for i, segment in enumerate(result.segments):
            # èªè­˜æˆåŠŸã®ã‚»ã‚°ãƒ¡ãƒ³ãƒˆã¯ç·‘ã€å¤±æ•—ã¯èµ¤ã§è¡¨ç¤º
            if segment['recognized_speaker'] != "æœªèªè­˜":
                status_color = "ğŸŸ¢"
                confidence_text = f" (ä¿¡é ¼åº¦: {segment['confidence']:.3f})"
            else:
                status_color = "ğŸ”´"
                confidence_text = ""
            
            with st.expander(
                f"{status_color} ã‚»ã‚°ãƒ¡ãƒ³ãƒˆ {segment['segment_id']}: "
                f"{segment['start_time']:.1f}s - {segment['end_time']:.1f}s "
                f"â†’ {segment['recognized_speaker']}{confidence_text}"
            ):
                col1, col2 = st.columns(2)
                with col1:
                    st.write(f"**é–‹å§‹æ™‚é–“**: {segment['start_time']:.1f}ç§’")
                    st.write(f"**çµ‚äº†æ™‚é–“**: {segment['end_time']:.1f}ç§’")
                    st.write(f"**æ™‚é–“é•·**: {segment['duration']:.1f}ç§’")
                with col2:
                    st.write(f"**ãƒ€ã‚¤ã‚¢ãƒ©ã‚¤ã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ãƒ©ãƒ™ãƒ«**: {segment['diarization_label']}")
                    st.write(f"**èªè­˜è©±è€…**: {segment['recognized_speaker']}")
                    if segment['confidence'] > 0:
                        st.write(f"**ä¿¡é ¼åº¦**: {segment['confidence']:.3f}")
        
        # è©±è€…åˆ¥ã‚µãƒãƒªãƒ¼
        st.subheader("ğŸ‘¥ è©±è€…åˆ¥ã‚µãƒãƒªãƒ¼")
        speaker_summary = {}
        for segment in result.segments:
            speaker = segment['recognized_speaker']
            if speaker not in speaker_summary:
                speaker_summary[speaker] = {
                    'segments': 0,
                    'total_time': 0.0,
                    'avg_confidence': 0.0
                }
            speaker_summary[speaker]['segments'] += 1
            speaker_summary[speaker]['total_time'] += segment['duration']
            if segment['confidence'] > 0:
                speaker_summary[speaker]['avg_confidence'] += segment['confidence']
        
        # å¹³å‡ä¿¡é ¼åº¦è¨ˆç®—
        for speaker in speaker_summary:
            if speaker_summary[speaker]['segments'] > 0:
                speaker_summary[speaker]['avg_confidence'] /= speaker_summary[speaker]['segments']
        
        # è¡¨å½¢å¼ã§è¡¨ç¤º
        summary_data = []
        for speaker, data in speaker_summary.items():
            summary_data.append({
                'è©±è€…': speaker,
                'ã‚»ã‚°ãƒ¡ãƒ³ãƒˆæ•°': data['segments'],
                'åˆè¨ˆæ™‚é–“': f"{data['total_time']:.1f}ç§’",
                'å¹³å‡ä¿¡é ¼åº¦': f"{data['avg_confidence']:.3f}" if data['avg_confidence'] > 0 else "N/A"
            })
        
        if summary_data:
            import pandas as pd
            df = pd.DataFrame(summary_data)
            st.dataframe(df, use_container_width=True, hide_index=True)
    
    else:
        st.warning("âš ï¸ ã‚»ã‚°ãƒ¡ãƒ³ãƒˆãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã‚„è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")

if __name__ == "__main__":
    main()